{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TD n°1 Deep Learning - Exo 1 FizzBuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On modélise le pb commme un pb de classification multi-classe.\n",
    "\n",
    "Les sorties possibles 0 / 1 / 2 / 3 correspondent respectivement \n",
    "* au nombre lui-même, \n",
    "* à fizz pour les multiples de 3, \n",
    "* buzz pour les multiples de 5,\n",
    "* fizzbuzz pour les multiples de 15."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Modélisation sortie de la vérité terrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fizz_buzz (i, prediction) :\n",
    "    return [str(i),..., ...,... ][prediction]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Création de la vérité terrain : [number, fizz, buzz, fizzbuzz]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fizz_buzz_encode(i):\n",
    "    # TODO return 0\n",
    "    # TODO return 1\n",
    "    # TODO return 2\n",
    "    # TODO return 3\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testez vos fonctions en vérifiant que vous renvoyez la bonne sortie pour $i=1,\\dots,19$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 20): \n",
    "    print(\n",
    "        i, \n",
    "        fizz_buzz_encode(i),# classe  \n",
    "        fizz_buzz(i , fizz_buzz_encode(i)) # classe -> résultat i ? fizz ? buzz ? fizzbuzz ?\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modélisation des entrées et des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encodage binaire \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_DIGITS = 10\n",
    "def binary_encode (i , num_digits=NUM_DIGITS ) :\n",
    "    return [... for d in range(num_digits)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_train, end_train = ..., ...\n",
    "start_test, end_test = ..., ...\n",
    "\n",
    "# train\n",
    "X_train = torch.FloatTensor ([ binary_encode (i , NUM_DIGITS ) for i in range (start_train, end_train)])\n",
    "Y_train = torch.LongTensor ([fizz_buzz_encode(i) for i in range(start_train, end_train)]).squeeze()\n",
    "\n",
    "# test\n",
    "X_test = torch.FloatTensor ([ binary_encode (i , NUM_DIGITS) for i in range (start_test, end_test) ])\n",
    "print(X_test[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création du modèle, de la loss et de l'optimiseur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nombre de neurones dans la couche cachée\n",
    "NUM_HIDDEN = 100\n",
    "\n",
    "# définition du MLP à 1 couche cachée (non linearite ReLU)\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(NUM_DIGITS, NUM_HIDDEN),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(NUM_HIDDEN, ...)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Fonction de coût__ \n",
    "\n",
    "CrossEntropyLoss vs NLLLoss ? (negative log likelihood)\n",
    "\n",
    "CE loss : The input is expected to contain the unnormalized logits for each class, *which do not need to be positive or sum to 1, in general*. \n",
    "\n",
    "NLLLoss : il faut avoir calculé un softmax\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fonction de coût\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimiseur --> choix du learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "raw_data_test = np.arange(1, 101) # valeurs de test\n",
    "\n",
    "for epoch in range(10000):\n",
    "    for start in range(0, len(X_train), BATCH_SIZE):\n",
    "        end = start + BATCH_SIZE\n",
    "        batchX = X_train[start:end]\n",
    "        batchY = Y_train[start:end]\n",
    "\n",
    "        # prediction et calcul de la loss\n",
    "        y_pred = model(batchX)\n",
    "        loss = loss_fn(y_pred, batchY)\n",
    "    \n",
    "        # mettre les gradients à 0 avant la passe retour (backward)\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        # rétro-propagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # calcul coût  (et affichage)\n",
    "    loss = loss_fn( model(X_train), Y_train)\n",
    "    if epoch%100 == 0:\n",
    "        print('epoch {} training loss {}'.format(epoch, round(loss.item(), 3)))\n",
    "\n",
    "    # visualisation des résultats en cours d'apprentissage\n",
    "    if(epoch%1000==0):\n",
    "        Y_test_pred = model(X_test)\n",
    "        val, idx = torch.max(Y_test_pred,1)\n",
    "        ii=idx.data.numpy()\n",
    "        # numbers = np.arange(1, 101)\n",
    "        output = np.vectorize(fizz_buzz)(raw_data_test, ii)\n",
    "        print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Affichage des résultats__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sortie finale (calcul lisible)\n",
    "Y_test_pred = model(X_test)\n",
    "val, idx = torch.max(Y_test_pred,1)\n",
    "ii=idx.data.numpy()\n",
    "output = np.vectorize(fizz_buzz)(raw_data_test, ii)\n",
    "print(\"============== Final result ============\")\n",
    "print(output)\n",
    "\n",
    "# Sortie finale (calcul plus compact des predictions)\n",
    "Y_test_pred = model(X_test)\n",
    "predictions = zip(range(1, 101), list(Y_test_pred.max(1)[1].data.tolist()))\n",
    "print(\"============== Final result ============\")\n",
    "print ([fizz_buzz(i, x) for (i, x) in predictions])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Calcul des performances (classification accuracy)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtY = np.array([fizz_buzz_encode(i) for i in raw_data_test])\n",
    "print(\"test perf: \", np.mean(gtY == ...))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Avec un validation set !__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_VAL = 100\n",
    "p = np.random.permutation(range(len(X_train)))\n",
    "# permettre de mélanger les nombres\n",
    "X_train, Y_train = X_train[..., :], Y_train[...]\n",
    "X_val, Y_val = X_train[..., :], Y_train[...]\n",
    "X_train, Y_train = X_train[...:, :], Y_train[...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "NUM_HIDDEN = 100\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(NUM_DIGITS, NUM_HIDDEN),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(NUM_HIDDEN, 4)\n",
    "    )\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.05)\n",
    "\n",
    "for epoch in range(10000):\n",
    "    for start in range(0, len(X_train), BATCH_SIZE):\n",
    "        end = start + BATCH_SIZE\n",
    "        batchX = X_train[start:end]\n",
    "        batchY = Y_train[start:end]\n",
    "\n",
    "        # prediction et calcul de la loss\n",
    "        y_pred = model(batchX)\n",
    "        loss = loss_fn(y_pred, batchY)\n",
    "    \n",
    "        # mettre les gradients à 0 avant la passe retour (backward)\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        # rétro-propagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # calcul coût  (et affichage)\n",
    "    loss = loss_fn( model(X_train), Y_train)\n",
    "    if epoch%100 == 0:\n",
    "        print('epoch {} training loss {}'.format(epoch, round(loss.item(), 3)))\n",
    "\n",
    "    # visualisation des résultats en cours d'apprentissage\n",
    "    # cette fois-ci sur l'ensemble de validation\n",
    "    if(epoch%1000==0):\n",
    "        # train acc\n",
    "        Y_train_pred = model(X_train)\n",
    "        print(\"train perf: \", np.mean(Y_train.data.numpy() == Y_train_pred.max(1)[1].data.numpy() ) )\n",
    "        Y_val_pred = model(X_val)\n",
    "        print(\"val perf: \", np.mean(Y_val.data.numpy() == Y_val_pred.max(1)[1].data.numpy() ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sortie finale (calcul lisible)\n",
    "Y_test_pred = model(X_test)\n",
    "val, idx = torch.max(Y_test_pred,1)\n",
    "ii=idx.data.numpy()\n",
    "output = np.vectorize(fizz_buzz)(raw_data_test, ii)\n",
    "print(\"============== Final result ============\")\n",
    "print(output)\n",
    "\n",
    "# Sortie finale (calcul plus compact des predictions)\n",
    "Y_test_pred = model(X_test)\n",
    "predictions = zip(range(1, 101), list(Y_test_pred.max(1)[1].data.tolist()))\n",
    "print(\"============== Final result ============\")\n",
    "print ([fizz_buzz(i, x) for (i, x) in predictions])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Experiment with different settings__\n",
    "\n",
    "* learning rate\n",
    "* optimizer\n",
    "* scheduler\n",
    "* number of training samples\n",
    "* architecture of the MLP (number of hidden units)\n",
    "* number of epochs\n",
    "* ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
