{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apprentissage profond - TD n°1\n",
    "__________\n",
    "\n",
    "#### Exo 3 : Apprentissage de portes logiques\n",
    "\n",
    "Implémentation de la règle de la chaîne pour un réseau linéaire à 1 couche cachée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Sigmoïde__\n",
    "\n",
    "Vérifier que la dérivée de la fonction sigmoïde $\\sigma$ revient à $\\sigma(1 - \\sigma)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Modélisation__\n",
    "\n",
    "On prend en entrée deux entiers (bits 0 ou 1), et on a un seul neurone de sortie (valeur entre 0 et 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X : données d'entrées --> 2 bits\n",
    "X = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "# on veut apprendre des règles logiques\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Implémentation d'une rétropropagation__\n",
    "\n",
    "Dans le cas d'un réseau simple (1 couche cachée, pas de biais)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# architecture du réseau : \n",
    "# entrée de taille 2\n",
    "# couche cachée à N neurones\n",
    "# sortie binaire, 1 neurone\n",
    "\n",
    "def logicNN(X,t): # X=data and t=ttarget (labels)\n",
    "    N = 4 # number of hidden units\n",
    "    epochs = 10000\n",
    "    lr = 0.1\n",
    "    W1 = np.random.rand(2,N) # initialisation aléatoire des poids, couche 1\n",
    "    W2 = np.random.rand(N,1) # idem couche 2\n",
    "    # NB : pas de biais utilisés ici\n",
    "\n",
    "    for e in range(epochs):\n",
    "        ### forward pass\n",
    "        out1 = sigmoid(np.dot(X,W1))\n",
    "        out2 = sigmoid(np.dot(out1,W2))\n",
    "\n",
    "        ### backprop\n",
    "        error = t - out2 # pas besoin de calculer explicitement la fonction de coup, pour la descente de gradient on utilise uniquement sa dérivée \n",
    "        # cf calcul de la règle de la chaîne / \"chain rule\"\n",
    "        d2 = 2 * error * (out2 * (1-out2)) # dLoss / dW2 à l'exception de *out_1\n",
    "        d1 = d2.dot(W2.T) * (out1*(1-out1)) # dLoss / dW1 à l'exception de *x\n",
    "        # SGD\n",
    "        W2 += lr * out1.T.dot(d2) # ici c'est un plus car error = -(y-t)\n",
    "        W1 += lr * X.T.dot(d1) # idem la mise à jour des paramètres revient bien à V(t+1) = V(t) - lr * dV(t)\n",
    "    return out2.flatten() #np.round(out2).flatten() pour la prédiction arrondie à l'entier le plus proche"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Application__\n",
    "\n",
    "On effectue l'apprentissage au moyen du réseau précédent, est-ce qu'on parvient à généraliser pour les portes OR, AND, NOR et NAND?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OR [0.03583178 0.98000211 0.98026932 0.99358967]\n",
      "OR [0. 1. 1. 1.]\n",
      "OR [0.03583178 0.98000211 0.98026932 0.99358967]\n",
      "OR [0. 1. 1. 1.]\n",
      "[0.00257954 0.97724329 0.9824299  0.99648479 0.04353473 0.98494695\n",
      " 0.97749218 0.99538127]\n",
      "OR [0. 1. 1. 1. 0. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Exemple du OR\n",
    "X = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "Y_pred = logicNN(X, np.array([[0,1,1,1]]).T)\n",
    "print(\"OR\", Y_pred)\n",
    "print(\"OR\", np.round(Y_pred))\n",
    "\n",
    "X = np.array([[0.1,0.1], \n",
    "              [0.2,0.9], \n",
    "              [0.8,0.15], \n",
    "              [0.85,0.8]])\n",
    "logicNN(X, np.array([[0,1,1,1]]).T)\n",
    "print(\"OR\", Y_pred)\n",
    "print(\"OR\", np.round(Y_pred))\n",
    "\n",
    "X = np.array([[0,0], \n",
    "              [0,1], \n",
    "              [1,0], \n",
    "              [1,1],\n",
    "              [0.1,0.1], \n",
    "              [0.2,0.9], \n",
    "              [0.8,0.15], \n",
    "              [0.85,0.8]])\n",
    "Y_pred = logicNN(X, np.array([[0,1,1,1,0,1,1,1]]).T)\n",
    "print(Y_pred)\n",
    "print(\"OR\", np.round(Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AND [0.00369704 0.02272587 0.02463533 0.96673854]\n",
      "XOR [0.06700654 0.95056136 0.95470503 0.03160883]\n",
      "NAND [0.99626988 0.97531799 0.97724513 0.03333575]\n",
      "NOR [0.95608561 0.02287015 0.02246966 0.01161282]\n"
     ]
    }
   ],
   "source": [
    "# Autre exemples\n",
    "X = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "print(\"AND\", logicNN(X, np.array([[0,0,0,1]]).T))\n",
    "print(\"XOR\", logicNN(X, np.array([[0,1,1,0]]).T))\n",
    "print(\"NAND\", logicNN(X, np.array([[1,1,1,0]]).T))\n",
    "print(\"NOR\", logicNN(X, np.array([[1,0,0,0]]).T))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
